---
title: "PSTAT174 Final Project"
author: "Joseph Chang"
date: "3/11/2022"
output:
  html_document:
    code_folding: hide
df_print: paged
---

```{r setup, include=FALSE, echo = FALSE}
# set global chunk options: images will be 7x5 inches
knitr::opts_chunk$set(fig.width=7, fig.height=5, echo =TRUE, message=FALSE, warning = FALSE)
options(digits = 4)
```

```{r}
# library functions
library(knitr)
library(tidyverse) 
library(ISLR) 
library(ROCR)
library(dplyr)
library(tinytex)
library(MASS)
devtools::install_github("FinYang/tsdl")
library(tsdl)
library(forecast)
library(astsa)
library(qpcR)
library(UnitCircle)
```

## Executive Summary

on own page

## Introduction

The data set I plan to use is the United States military defense budget since 1960. This is essentially money the US military has used or uses for its own defense and protecting its citizens against other countries/people. Although the dataset has two other columns dedicated for population of the U.S. and GDP for each respective year, I find the defense budget column most interesting because it matches closely with the history of the United States and the wars it has participated in. Additionally, GDP and population followed a constant, linear relationship, so it was easy to predict and not as suitable for a detailed time series project.

This data is interesting to me because aside from studying statistics and data science, I have long been intrigued with history, especially US history and all the wars it has fought or participated in. This data provides me with an idea of how much the US government spent during the wars and also in the present day. For example, based from looking at the plot, I can infer that during the Vietnam War (1955-1975), the government consistently spent money to protect its own borders, and it wasn't until after 9/11 (2001) that the government began to intensify and exponentiate their spending in the years subsequently after and during the Iraq War. It wasn't until 2011 when Osama bin Laden's death led to the US to decrease their funding.

I plan to split use the original dataset to transform into another dataset. Using this new updated dataset, I will split the data into training and testing. I will use the training dataset to predict and forecast the model and compare it with the testing dataset. In the end, I will compare the testing dataset with the original dataset to make conclusions

## Data Importation

First, I read in the data and plotted a time series model for the Defense Budget each of the years, along with a line for trend in red and the mean in blue. Additionally, I created a histogram to visualize the entire Defense Budget dataset.

```{r}
spending_data <- read.csv("/Users/josephchang/Desktop/MilitarySpending.csv.xls")

# added line for trend 
plot.ts(spending_data$DefenseBudget, ylab = "Defense budget (in US billion)", xlab = "Year after 1960")
nt = length(spending_data$DefenseBudget)
fit <- lm(spending_data$DefenseBudget ~ as.numeric(1:nt))
abline(fit, col ="red")
# added mean
data_mean <- mean(spending_data$DefenseBudget)
abline(h=data_mean, col="blue")

# histogram of original data points
hist(spending_data$DefenseBudget, label=TRUE, main = "Histogram of U.S Defense Budget", breaks="Sturges", xlab = "Defense Budget (in US billion)")

```

Immediate observations: There seems to be a linear trend that is positive but there is no seasonality and no apparent sharp change in behavior. There is non-constant variance and mean. The histogram seems to be skewed right


## Testing and training for the original data

I created training and testing sets for spending_data$DefenseBudget, with the last 6 datapoints as the testing set. I chose 6 datapoints because 10% of the observations is around 6.1.

For visualization, I plotted the time series plot for the training set, along with trend and mean.

```{r}
# Define training and testing sets
training = spending_data$DefenseBudget[c(1:51)]
testing  = spending_data$DefenseBudget[c(52:61)]

# time series plot in training with trend and mean
plot.ts(training, ylab = "Defense budget (in US billion)", xlab = "Year after 1960")
fit <- lm(training ~ as.numeric(1:length(training)))
abline(fit, col="red")
abline(h=mean(training), col = "blue")
```


# Transformation

Next, since the original data looked skewed, I wanted to see if I needed to make any necessary transformations to make the model stationary and to stabilize the variance.

```{r}
t <- 1:length(training)
fit <- lm(training~t)
bcTransform <- boxcox(training ~ t, plotit=TRUE)
lambda = bcTransform$x[which(bcTransform$y == max(bcTransform$y))]
training.bc = (1/lambda)*(training^lambda-1)
```

Using the Box-Cox transformation, the BcTransform command gave me the value of lambda to be around 0.1414. Then I named the box cox transformation data training.bc. Note: I do notice that 0 is in the confidence interval.

```{r}
# comparison of original, boxcox transformation, and log transformation
training.log = log(training)

# compare transforms on time series plot
op=par(mfrow=c(2,2))
plot.ts(training, main = "Original Training set")
plot.ts(training.bc, main = "Box-cox Transform")
plot.ts(training.log, main = "Log transform")
```

For comparison, I created log and square root transformations as well. In order to determine which model to use, I will create histograms of the original model vs the transformed model (spent).

```{r}
# compare transforms on histogram
hist(training, col = "light blue", main = "Histogram of trained Original Data", breaks = "Sturges")
hist(training.bc, col = "light blue", main = "Histogram of Boxcox transformed Data", breaks="Sturges")
hist(training.log, col = "light blue", main = "Histogram of Log transformed Data", breaks="Sturges")
```

Based from the histogram, the original is still looking skewed right. The boxcox transformation looks more symmetric. Log transformation, to me, looks even more symmetric than  Variance also looked more stable in the transformation. As a result, the log transformation looks the most appropriate to use.

## Differencing

Then, I want to check if training.log needed any differencing. To start, I differenced the log of training set, then at d=2, and d=3. All differencing took place at lag 1. 

```{r}
dat.1 <- diff(training.log, 1)
dat.2 <- diff(dat.1, 1)
dat.3 <- diff(dat.2, 1)

# check variances
var(training.log)
var(dat.1)
var(dat.2)
var(dat.3)
```

To determine when to stop, I checked the variances for the 3 differences. I concluded that I only need to difference when d=1 because variance is lowest when d=1. At d=2, variance increased again. Therefore, d=1 is most appropriate.

\\
I plotted the time series for dat.1 along with its mean and trend for comparison and its histogram

```{r}
# time series plot for dat.1
plot.ts(dat.1, main = "log training when d=1", type= "l")
fit_a <- lm(dat.1 ~ as.numeric(1:length(dat)))
abline(fit_a, col = "red")
abline(h=mean(dat.1), col = "blue")

# histograms
hist(dat.1, col = "light blue", xlab = "", main = "Histogram at d=1")
m <- mean(dat.2)
std <- sqrt(var(dat.2))
curve(dnorm(x,m,std), add = T)
```

The time series plot for dat.1 looks somewhat stationary and the histogram looks somewhat gaussian.


## Model Identification

For comparison, I plotted ACF and PACF for training.log and dat.1. I will compare histograms of the training, dat.1. I set maximum lag at 40 because since there is no seasonality in training, I won't need an excessive amount of data.

```{r}
# acf, pacf for training.log
par(mar=c(0.75,3,3,0.75))
acf(training.log , lag.max=40, main = "ACF of training.log", ylim=c(-1,1))
pacf(training.log, lag.max=40, main = "PACF of training.log", ylim=c(-1,1))

# acf, pacf for dat.1
acf(dat.1, lag.max = 40, main="ACF of difference once", ylim=c(-1,1),xlab="h", ylab= expression(hat(rho)[X](h)))
pacf(dat.1, lag.max = 40, main="PACF of difference once", ylim=c(-1,1),xlab="h", ylab= expression(hat(rho)[X](h)))

```

In dat.1, I noticed that for ACF, lags 1,9,10,11 were outside of the confidence interval, with lag 9 being the furthest outside. For PACF, both lags 1 and 9 outside confidence interval, with 9 again being the furthest and highest lag to be outside.


## Model Estimation

Here, I will estimate coefficients of p and q. I used a for loop function to find the lowest AICc. This for loop will produce candidate models for me to run diagnostics on. 
\\
Since the highest lag I saw on the ACF for dat.1 was 11, the loop will take into consideration values of q from 0 to 11. For PACF, it is from 0 to 9.

```{r}
df <- expand.grid(p=0:9, q=0:11, P=0:1, Q=0:1) 
df <- cbind(df, AICc=NA)

for (i in 1:nrow(df)) {
  sarima.obj <- NULL
  try(arima.obj <- arima(training.log, order=c(df$p[i],1, df$q[i]), method="ML"))
  if (!is.null(arima.obj)) { df$AICc[i] <- AICc(arima.obj) }
  # print(df[i, ])
  
  # seasonal =  list(order=c(df$P[i], 0, df$Q[i]), period=9),
  
  
}
# this prints all AICc values, starting from the lowest to highest
head(df[order(df$AICc),], 5)
```

Based from the lowest AICc, I will consider the two best models, which are (1,0) and (0,8), where p and q are represented respectively. 
\\

Since the ACF of dat.1 showed lags 1, 9, 10,11 to be outside the confidence interval as well as PACF outside at lag 9, I suspected there might be some potential use of the SARIMA model, with period at 9. I tested this on fit_test with order as (9,1,0). The results showed me that coefficients could be zero from ar2 to ar8. This led me to create new_fit_test to reduce the model with fixed values of zero from ar2 to ar8. After comparing the AICc of fit_test and new_fit_test, I determined that SARIMA model was needed.

```{r}
fit_test <- arima(training.log, order = c(9,1,0), method = "ML")
fit_test
AICc(fit_test)

new_fit_test <- arima(training.log, order = c(9,1,0),fixed=c(NA,0,0,0,0,0,0,0,NA), method = "ML")
new_fit_test
AICc(new_fit_test)
```

I first tested fit1 at (1,1,0) like before because it produced the lowest AICC. Then, I created new_fit1 to reduce the model with period at 9 and p = P, which was 1. Indeed, the new_fit1 produced a lower AICc then fit1.


```{r}
# Final model 1
fit1 <- arima(training.log, order = c(1,1,0), method = "ML")
fit1
AICc(fit1)

new_fit1 <- arima(training.log, order = c(1,1,0), seasonal = list(order = c(1,0,0), period = 9), method = "ML")
new_fit1
AICc(new_fit1)

```
I determined that new_fit1 or model 1 is stationary because its phi value is less than the absolute value of 1 and all AR models are invertible.

\\
Second, I test fit2 at (0,1,8) because it produced the second-lowest AICc. Since fit2 has no AR part, I created new_fit2 to find which coefficents were 0, which was only at ma3. This led me to create a SARIMA model for fit2 called new_fit22. After comparison of fit2, new_fit2, and new_fit22, new_fit22 produced lowest AICc.

```{r}
# Final model 2
fit2 <- arima(training.log, order = c(0,1,8), method = "ML")
fit2
AICc(fit2)


new_fit2 <- arima(training.log, order = c(0,1,8), fixed = c(0,0,NA,0,0,0,0,0), method = "ML")
new_fit2
AICc(new_fit2)


new_fit22 <- arima(training.log, order = c(0,1,8), seasonal = list(order = c(1,0,0), period = 9), method = "ML")
new_fit22
AICc(new_fit22)


# check invertiblity of model 2
uc.check(pol_ = c(0,0,0.311, 0, 0, 0, 0, 0), plot_output = T)

```

Model 2 is stationary because it is purely MA. Model 2 is invertible because all roots are outside unit circle.

## Model Diagnostics for model 1

This will be model diagnostics for model 1: order = (1,1,0). For testing the independence of residuals, lag 8 is chosen because square root of observations (61) is around 8.

```{r}
# residual plots
res <- residuals(new_fit1)
mean(res)
var(res)

# layout
par(mfrow=c(1,1))
ts.plot(res, main  = "Fitted Residuals")
t <- 1:length(res)
new_fit1.res = lm(res~t)
abline(new_fit1.res)
abline(h=mean(res), col = "blue")

# Testing independence
Box.test(res, lag = 8, type = c("Box-Pierce"), fitdf = 1)
Box.test(res, lag = 8, type = c("Ljung-Box"), fitdf = 1)
Box.test(res^2, lag = 8, type = c("Ljung-Box"), fitdf = 0)

# test for normality of residuals
shapiro.test(res)

# Histogram and qq plot
par(mfrow=c(1,2))
hist(res, main= "Histogram")
qqnorm(res)
qqline(res, col = "blue")
```

Model 1 passes all tests and residuals are normal. ACF of residuals^2 shows nonlinear dependence

```{r}
# ACF and PACF of residuals
par(mfrow=c(1,2))
acf(res, main = "Autocorrelation", lag.max = 40)
pacf(res, main = "Partial Autocorrelation", lag.max = 40)
```

The ACF and PACF of residuals shows no lags outside of confidence interval. The fitted residuals are to AR(0), so it is white noise. This passes diagnostic checking for model 1.



## Model Diagnostics for model 2

This will be model diagnostics for model 1: order = (0,1,8)

```{r}
# residual plots
res2 <- residuals(fit2)
mean(res2)
var(res2)

# layout
par(mfrow=c(1,1))
ts.plot(res2, main  = "Fitted Residuals")
t <- 1:length(res2)
fit2.res2 = lm(res2~t)
abline(fit2.res2)
abline(h=mean(res2), col = "blue")

# Testing for independence of residuals
Box.test(res2, lag = 8, type = c("Box-Pierce"), fitdf = 8)
Box.test(res2, lag = 8, type = c("Ljung-Box"), fitdf = 8)
Box.test(res2^2, lag = 8, type = c("Ljung-Box"), fitdf = 0)

# test for normality of residuals
shapiro.test(res2)

# Histogram and qq plot
par(mfrow=c(1,2))
hist(res2, main= "Histogram")
qqnorm(res2)
qqline(res2, col = "blue")
```

Model 2 does not pass the tests for independence of residuals, but residuals are normal. Therefore, model 2 cannot be used. 



## Best model

Anaylsis of residuals satisfactory?? Since only model 1 passed  passed diagnostic checking and has a satisfactory analysis of residuals, I will use model 1.

```{r}
new_fit1
```

Final model should be $(1-0.831B)(1+0.566B^9)(1-B)X_t = Z_t, Z_t ~ WN(0, 0.00271)$

## Data Forecasting

In the final step of forecasting, I will predict 10 future observations (colored in red) and plot it against the testing set of the original data (colored in blue) for comparison. I will do the same for the transformed data. For fun, I predicted 5 years my prediction in the original data.

```{r}
# Predict 10 future observations on data and plot on original
m <- length(training)
mypred <- predict(new_fit1, n.ahead=15)

ts.plot(training, xlim=c(1, length(training)+15), ylim=c(min(training), 1000) , xlab = "Years after 1960", ylab = "Billions of USD")

points((m+1):(m+15), col="blue", exp(mypred$pred))
points((m+1):(m+10), col="red", testing)
lines((m+1):(m+10), col="black", testing, lty= "solid")
lines((exp(mypred$pred +2*mypred$se)), col = "blue", lty= "dashed")
lines((exp(mypred$pred -2*mypred$se)), col = "blue", lty= "dashed")


# predict 10 future observations and plot on transformed data
m <- length(training.log)
mypred <- predict(new_fit1, n.ahead=10)

ts.plot(training.log, xlim=c(1, length(training.log)+10), ylim=c(min(training.log), max(mypred$pred +2*mypred$se)) , xlab = "Years after 1960", ylab = "Billions of USD")

points((m+1):(m+10), col="blue", mypred$pred)
lines((mypred$pred +2*mypred$se), col = "blue", lty= "dashed")
lines((mypred$pred -2*mypred$se), col = "blue", lty= "dashed")
```

## Conclusion

Model 1 and 2 both passed diagnostic checking but .. Ultimately, my predicted values followed a straight linear line that would have shown increased defense funding. But, the testing dataset pointed a curve instead. Individuals who helped me on the project include Sunpeng, Youhong, and Raisa Feldman.

## References

I used this site to obtain my original dataset: <https://www.kaggle.com/brandonconrady/us-military-spending-by-year-1960-2020/version/1>

## Appendix

```{r get-labels, echo = FALSE}
labs = knitr::all_labels()
labs = setdiff(labs, c("setup", "get-labels"))
```

```{r all-code, ref.label=labs, eval=FALSE}
```
