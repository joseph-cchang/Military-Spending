---
title: "PSTAT174 Final Project"
author: "Joseph Chang"
date: "3/11/2022"
output:
  html_document:
    code_folding: hide
df_print: paged
---

```{r setup, include=FALSE, echo = FALSE}
# set global chunk options: images will be 7x5 inches
knitr::opts_chunk$set(fig.width=7, fig.height=5, echo =TRUE, message=FALSE, warning = FALSE)
options(digits = 4)
```

```{r}
# library functions
library(knitr)
library(tidyverse) 
library(ISLR) 
library(ROCR)
library(dplyr)
library(tinytex)
library(MASS)
#devtools::install_github("FinYang/tsdl")
library(tsdl)
library(forecast)
library(astsa)
library(qpcR)
library(UnitCircle)
```

## Executive Summary

This project focuses on the United States’ defense budget from the years 1960 ~ 2021. The goal of this project was to create a model that could predict the last 10 points (years) of the U.S. Defense Budget and compare it with actual last 10 points (years) of the data.  In order to do this, I began by splitting the original dataset into testing and training sets, with training as the first 51 points and testing as the last 10 points. As described earlier, this allows the training data to predict the future points and compare it with the testing data at the end. As a result, the training data, not the entirety of the original model, is used throughout the project. 
After training, I made any necessary transformations using Boxcox. I discovered that I needed to transform the training data by taking its log. Next, when I differenced and found smallest variance, I had to difference the log of training at lag 1 once. From there, I could plot the ACF and PACF of my training data and estimate the parameters of the model using a for loop that produced the lowest AICc of the potential models. After model estimation, I considered the two best models from the lowest AICc and began model diagnostics. In this step, I tested if the residuals in my candidate models were independent, had any nonlinear dependence, and were normal. If passed, the model could be considered for forecasting. In the model forecasting, I predicted the 10 future observations from my best model and concluded that my model closely followed the actual model for about the first seven points (years), but then began to diverge from the actual model the rest of the way. As a last step, I predicted 5 extra years from my model in the original dataset just for my curiosity. From this project, I learned that the United States government spends a tremendous amount of money (billions of dollars) on its defense budget.


## Introduction

The data I used shows the United States military defense budget since 1960. The data has a yearly frequency and measures budget spending in terms of billions of USD. Using the website Kaggle, I imported a csv file into RStudios and began to analyze the time series data. Although the original dataset has two other columns dedicated to population and GDP of US for each respective year, I find the defense budget most interesting it matches closely with the history of the United States and the wars it has participated in. For example, after the fall of USSR (1992) and ending the Cold War, US defense spending dipped, and stayed constant. But after 9/11attacks (2001), total spending exponentiated as the government prioritized protecting its own citizens. Both of these historical events are corroborated by the data. Thus, the goal of this project is to create a model that could predict future US spending as close to possible. To solve this problem, I split training and testing sets from the original model, with testing as the last 10 points. Using the training set to predict parameters in my model, I could determine a best model that would represent the trained dataset. In the forecasting step, I plotted a time series on the trained set and predicted 15 steps ahead: 10 for comparison with testing (marked in red) and 5 just for my pleasure (marked in blue). Along with these points were 95 % confidence intervals that marked the margin of error for the predicted points. Both the predicted points and confidence intervals are in blue to signify prediction. The results indicated that the confidence intervals were very wide, meaning margin of error could be large. While the first 7 predicted points were closely related to first 7 actual points, the rest of predicted model showed a very different trend compared to the testing set. I can conclude that if my model were to continue predicting beyond the testing set, it would probably not be very accurate. 


## Data Importation

First, I read in the data and plotted a time series model for the Defense Budget each of the years, along with a line for trend in red and the mean in blue. Additionally, I created a histogram to visualize the entire Defense Budget dataset.

```{r}
spending_data <- read.csv("/Users/josephchang/Desktop/MilitarySpending.csv.xls")

# added line for trend 
plot.ts(spending_data$DefenseBudget, ylab = "Defense budget (in US billion)", xlab = "Year after 1960")
nt = length(spending_data$DefenseBudget)
fit <- lm(spending_data$DefenseBudget ~ as.numeric(1:nt))
abline(fit, col ="red")
# added mean
data_mean <- mean(spending_data$DefenseBudget)
abline(h=data_mean, col="blue")

# histogram of original data points
hist(spending_data$DefenseBudget, label=TRUE, main = "Histogram of U.S Defense Budget", breaks="Sturges", xlab = "Defense Budget (in US billion)")

```

Immediate observations: There seems to be a linear trend that is positive but there is no seasonality and no apparent sharp change in behavior. There is non-constant variance and mean. The histogram seems to be skewed right

## Testing and training for the original data

I created training and testing sets for spending_data\$DefenseBudget.

For visualization, I plotted the time series plot for the training set, along with trend and mean.

```{r}
# Define training and testing sets
training = spending_data$DefenseBudget[c(1:51)]
testing  = spending_data$DefenseBudget[c(52:61)]

# time series plot in training with trend and mean
plot.ts(training, ylab = "Defense budget (in US billion)", xlab = "Year after 1960")
fit <- lm(training ~ as.numeric(1:length(training)))
abline(fit, col="red")
abline(h=mean(training), col = "blue")
```

# Transformation

Next, since the original data looked skewed, I wanted to see if I needed to make any necessary transformations to make the model stationary and to stabilize the variance.

```{r}
t <- 1:length(training)
fit <- lm(training~t)
bcTransform <- boxcox(training ~ t, plotit=TRUE)
lambda = bcTransform$x[which(bcTransform$y == max(bcTransform$y))]
training.bc = (1/lambda)*(training^lambda-1)
```

Using the Box-Cox transformation, the BcTransform command gave me the value of lambda to be around 0.1414. Then I named the box cox transformation data training.bc. Note: I do notice that 0 is in the confidence interval.

```{r}
# comparison of original, boxcox transformation, and log transformation
training.log = log(training)

# compare transforms on time series plot
op=par(mfrow=c(2,2))
plot.ts(training, main = "Original Training set")
plot.ts(training.bc, main = "Box-cox Transform")
plot.ts(training.log, main = "Log transform")
```

For comparison, I created log and square root transformations as well. In order to determine which model to use, I will create histograms of the original model vs the transformed model (spent).

```{r}
# compare transforms on histogram
hist(training, col = "light blue", main = "Histogram of trained Original Data", breaks = "Sturges")
hist(training.bc, col = "light blue", main = "Histogram of Boxcox transformed Data", breaks="Sturges")
hist(training.log, col = "light blue", main = "Histogram of Log transformed Data", breaks="Sturges")
```

Based from the histogram, the original is still looking skewed right. The boxcox transformation looks more symmetric. Log transformation, to me, looks even more symmetric than Variance also looked more stable in the transformation. As a result, the log transformation looks the most appropriate to use.

## Differencing

Then, I want to check if training.log needed any differencing. To start, I differenced the log of training set, then at d=2, and d=3. All differencing took place at lag 1.

```{r}
dat.1 <- diff(training.log, 1)
dat.2 <- diff(dat.1, 1)
dat.3 <- diff(dat.2, 1)

# check variances
var(training.log)
var(dat.1)
var(dat.2)
var(dat.3)
```

To determine when to stop, I checked the variances for the 3 differences. I concluded that I only need to difference when d=1 because variance is lowest when d=1. At d=2, variance increased again. Therefore, d=1 is most appropriate.

\\ I plotted the time series for dat.1 along with its mean and trend for comparison and its histogram

```{r}
# time series plot for dat.1
plot.ts(dat.1, main = "log training when d=1", type= "l")
fit_a <- lm(dat.1 ~ as.numeric(1:length(dat.1)))
abline(fit_a, col = "red")
abline(h=mean(dat.1), col = "blue")

# histograms
hist(dat.1, col = "light blue", xlab = "", main = "Histogram at d=1")
m <- mean(dat.2)
std <- sqrt(var(dat.2))
curve(dnorm(x,m,std), add = T)
```

The time series plot for dat.1 looks somewhat stationary and the histogram looks somewhat gaussian.

## Model Identification

For comparison, I plotted ACF and PACF for training.log and dat.1. I will compare histograms of the training, dat.1. I set maximum lag at 40 because since there is no seasonality in training, I won't need an excessive amount of data.

```{r}
# acf, pacf for training.log
par(mar=c(0.75,3,3,0.75))
acf(training.log , lag.max=40, main = "ACF of training.log", ylim=c(-1,1))
pacf(training.log, lag.max=40, main = "PACF of training.log", ylim=c(-1,1))

# acf, pacf for dat.1
acf(dat.1, lag.max = 40, main="ACF of difference at lag 1 once", ylim=c(-1,1),xlab="h", ylab= expression(hat(rho)[X](h)))
pacf(dat.1, lag.max = 40, main="PACF of difference at lag 1 once", ylim=c(-1,1),xlab="h", ylab= expression(hat(rho)[X](h)))

```

In dat.1, I noticed that for ACF, lags 1,9,10,11 were outside of the confidence interval, with lag 9 being the furthest outside. For PACF, both lags 1 and 9 outside confidence interval, with 9 again being the furthest and highest lag to be outside.

## Model Estimation

Here, I will estimate coefficients of p and q. I used a for loop function to find the lowest AICc. This for loop will produce candidate models for me to run diagnostics on. \\ Since the highest lag I saw on the ACF for dat.1 was 11, the loop will take into consideration values of q from 0 to 11. For PACF, it is from 0 to 9.

```{r}
df <- expand.grid(p=0:9, q=0:11)
# , P=0:1, Q=0:1
df <- cbind(df, AICc=NA)

for (i in 1:nrow(df)) {
  sarima.obj <- NULL
  try(arima.obj <- arima(training.log, order=c(df$p[i],1, df$q[i]), method="ML"))
  if (!is.null(arima.obj)) { df$AICc[i] <- AICc(arima.obj) }
  # print(df[i, ])
  
  # seasonal =  list(order=c(df$P[i], 0, df$Q[i]), period=9),
  
  
}
# this prints all AICc values, starting from the lowest to highest
head(df[order(df$AICc),], 5)
```

Based from the lowest AICc, I will consider the two best models, which are (1,0) and (0,8), where p and q are represented respectively. \\

Since the ACF of dat.1 showed lags 1, 9, 10,11 to be outside the confidence interval as well as PACF outside at lag 9, I suspected there might be some potential use of the SARIMA model, with period at 9. I tested this on fit_test with order as (9,1,0). The results showed me that coefficients could be zero from ar2 to ar8. This led me to create new_fit_test to reduce the model with fixed values of zero from ar2 to ar8. After comparing the AICc of fit_test and new_fit_test, I determined that SARIMA model was needed.

```{r}
fit_test <- arima(training.log, order = c(9,1,0), method = "ML")
fit_test
AICc(fit_test)

new_fit_test <- arima(training.log, order = c(9,1,0),fixed=c(NA,0,0,0,0,0,0,0,NA), method = "ML")
new_fit_test
AICc(new_fit_test)
```

I first tested fit1 at (1,1,0) like before because it produced the lowest AICC. Then, I created new_fit1 to reduce the model with period at 9 and p = P, which was 1. Indeed, the new_fit1 produced a lower AICc then fit1.

```{r}
# Final model 1
fit1 <- arima(training.log, order = c(1,1,0), method = "ML")
fit1
AICc(fit1)

new_fit1 <- arima(training.log, order = c(1,1,0), seasonal = list(order = c(1,0,0), period = 9), method = "ML")
new_fit1
AICc(new_fit1)

```

I determined that new_fit1 or model 1 is stationary because its phi value is less than the absolute value of 1 and all AR models are invertible.

\\ Second, I test fit2 at (0,1,8) because it produced the second-lowest AICc. Since fit2 has no AR part, I created new_fit2 to find which coefficents were 0, which was only at ma3. This led me to create a SARIMA model for fit2 called new_fit22. After comparison of fit2, new_fit2, and new_fit22, new_fit22 produced lowest AICc.

```{r}
# Final model 2
fit2 <- arima(training.log, order = c(0,1,8), method = "ML")
fit2
AICc(fit2)


new_fit2 <- arima(training.log, order = c(0,1,8), fixed = c(0,0,NA,0,0,0,0,0), method = "ML")
new_fit2
AICc(new_fit2)


new_fit22 <- arima(training.log, order = c(0,1,8), seasonal = list(order = c(1,0,0), period = 9), method = "ML")
new_fit22
AICc(new_fit22)


# check invertiblity of model 2
uc.check(pol_ = c(0,0,0.311, 0, 0, 0, 0, 0), plot_output = T)

```

Model 2 is stationary because it is purely MA. Model 2 is invertible because all roots are outside unit circle.

## Model Diagnostics for model 1

This will be model diagnostics for model 1: order = (1,1,0). For testing the independence of residuals, lag 8 is chosen because square root of observations (61) is around 8.

```{r}
# residual plots
res <- residuals(new_fit1)
mean(res)
var(res)

# layout
par(mfrow=c(1,1))
ts.plot(res, main  = "Fitted Residuals")
t <- 1:length(res)
new_fit1.res = lm(res~t)
abline(new_fit1.res)
abline(h=mean(res), col = "blue")

# Testing independence
Box.test(res, lag = 8, type = c("Box-Pierce"), fitdf = 1)
Box.test(res, lag = 8, type = c("Ljung-Box"), fitdf = 1)
Box.test(res^2, lag = 8, type = c("Ljung-Box"), fitdf = 0)

# test for normality of residuals
shapiro.test(res)

# Histogram and qq plot
par(mfrow=c(1,2))
hist(res, main= "Histogram")
qqnorm(res)
qqline(res, col = "blue")
```

Model 1 passes all tests and residuals are normal. ACF of residuals\^2 shows nonlinear dependence

```{r}
# ACF and PACF of residuals
par(mfrow=c(1,2))
acf(res, main = "Autocorrelation", lag.max = 40)
pacf(res, main = "Partial Autocorrelation", lag.max = 40)
```

The ACF and PACF of residuals shows no lags outside of confidence interval. The fitted residuals are to AR(0), so it is white noise. This passes diagnostic checking for model 1.

## Model Diagnostics for model 2

This will be model diagnostics for model 1: order = (0,1,8)

```{r}
# residual plots
res2 <- residuals(fit2)
mean(res2)
var(res2)

# layout
par(mfrow=c(1,1))
ts.plot(res2, main  = "Fitted Residuals")
t <- 1:length(res2)
fit2.res2 = lm(res2~t)
abline(fit2.res2)
abline(h=mean(res2), col = "blue")

# Testing for independence of residuals
Box.test(res2, lag = 8, type = c("Box-Pierce"), fitdf = 8)
Box.test(res2, lag = 8, type = c("Ljung-Box"), fitdf = 8)
Box.test(res2^2, lag = 8, type = c("Ljung-Box"), fitdf = 0)

# test for normality of residuals
shapiro.test(res2)

# Histogram and qq plot
par(mfrow=c(1,2))
hist(res2, main= "Histogram")
qqnorm(res2)
qqline(res2, col = "blue")
```

Model 2 does not pass the tests for independence of residuals, but residuals are normal. Therefore, model 2 cannot be used.

## Best model

Since only the analysis of residuals in model 1 was satisfactory and passed diagnostic checking, I will use model 1.

```{r}
new_fit1
```

Final model should be $(1-0.831B)(1+0.566B^9)(1-B)X_t = Z_t, Z_t ~ WN(0, 0.00271)$

## Data Forecasting

In the final step of forecasting, I will predict 10 future observations (colored in red) and plot it against the testing set of the original data (colored in blue) for comparison. I will do the same for the transformed data. For fun, I predicted 5 years my prediction in the original data.

```{r}
# Predict 10 future observations on data and plot on original
m <- length(training)
mypred <- predict(new_fit1, n.ahead=15)

ts.plot(training, xlim=c(1, length(training)+15), ylim=c(min(training), 1000) , xlab = "Years after 1960", ylab = "Billions of USD")

points((m+1):(m+15), col="blue", exp(mypred$pred))
points((m+1):(m+10), col="red", testing)
lines((m+1):(m+10), col="black", testing, lty= "solid")
lines((exp(mypred$pred +2*mypred$se)), col = "blue", lty= "dashed")
lines((exp(mypred$pred -2*mypred$se)), col = "blue", lty= "dashed")


# predict 10 future observations and plot on transformed data
m <- length(training.log)
mypred <- predict(new_fit1, n.ahead=10)

ts.plot(training.log, xlim=c(1, length(training.log)+10), ylim=c(min(training.log), max(mypred$pred +2*mypred$se)) , xlab = "Years after 1960", ylab = "Billions of USD")

points((m+1):(m+10), col="blue", mypred$pred)
lines((mypred$pred +2*mypred$se), col = "blue", lty= "dashed")
lines((mypred$pred -2*mypred$se), col = "blue", lty= "dashed")
```

## Conclusion

Model 1 and 2 both passed diagnostic checking but .. Ultimately, my predicted model followed closely with the actual model at first, but then became very different towards the end. The goal of creating a model that could predict future values of the original model was achieved. However, it is not very accurate, especially at predicting in the long-term future. Therefore, I would determine not to use my model for predicting defense budget in the future. My model consisted of this formula : (1−0.831B)(1+0.566B^9)(1−B)Xt = Zt . I would like to acknowledge Raisa Feldman, Youhong Lee, and Sunpeng Duan for their contributions and assistance with this project and thank them for taking the time, effort, and energy out of their busy schedules to help me. 


## References

I used this site to obtain my original dataset: <https://www.kaggle.com/brandonconrady/us-military-spending-by-year-1960-2020/version/1>

## Appendix

```{r get-labels, echo = FALSE}
labs = knitr::all_labels()
labs = setdiff(labs, c("setup", "get-labels"))
```

```{r all-code, ref.label=labs, eval=FALSE}
```
