---
title: "PSTAT174 Final Project"
author: "Joseph Chang"
date: "3/11/2022"
output:
  html_document:
    code_folding: hide
df_print: paged
---

```{r setup, include=FALSE, echo = FALSE}
# set global chunk options: images will be 7x5 inches
knitr::opts_chunk$set(fig.width=7, fig.height=5, echo =TRUE, message=FALSE, warning = FALSE)
options(digits = 4)
```

```{r}
# library functions
library(knitr)
library(tidyverse) 
library(ISLR) 
library(ROCR)
library(dplyr)
library(tinytex)
library(MASS)
devtools::install_github("FinYang/tsdl")
library(tsdl)
library(forecast)
library(astsa)
library(qpcR)
```

## Executive Summary

on own page

## Introduction

The data set I plan to use is the United States military defense budget since 1960. This is essentially money the US military has used or uses for its own defense and protecting its citizens against other countries/people. Although the dataset has two other columns dedicated for population of the U.S. and GDP for each respective year, I find the defense budget column most interesting because it matches closely with the history of the United States and the wars it has participated in. Additionally, GDP and population followed a constant, linear relationship, so it was easy to predict and not as suitable for a detailed time series project.

This data is interesting to me because aside from studying statistics and data science, I have long been intrigued with history, especially US history and all the wars it has fought or participated in. This data provides me with an idea of how much the US government spent during the wars and also in the present day. For example, based from looking at the plot, I can infer that during the Vietnam War (1955-1975), the government consistently spent money to protect its own borders, and it wasn't until after 9/11 (2001) that the government began to intensify and exponentiate their spending in the years subsequently after and during the Iraq War. It wasn't until 2011 when Osama bin Laden's death led to the US to decrease their funding.

I plan to split use the original dataset to transform into another dataset. Using this new updated dataset, I will split the data into training and testing. I will use the training dataset to predict and forecast the model and compare it with the testing dataset. In the end, I will compare the testing dataset with the original dataset to make conclusions

## Data Importation

First, I read in the data and plotted a time series model for the Defense Budget each of the years, along with a line for trend in red and the mean in blue. Additionally, I created a histogram to visualize the entire Defense Budget dataset.

```{r}
spending_data <- read.csv("/Users/josephchang/Desktop/MilitarySpending.csv.xls")

plot(spending_data$Year, spending_data$DefenseBudget, ylab = "Defense budget (in US billion)", xlab = "Year", type="l")

plot.ts(spending_data$Year, spending_data$DefenseBudget, ylab = "Defense budget (in US billion)", xlab = "Year", type="l")
nt = length(spending_data$DefenseBudget)
fit <- lm(spending_data$DefenseBudget ~ as.numeric(1:nt))
abline(fit, col ="red")

# added trend 
plot.ts(spending_data$DefenseBudget, ylab = "Defense budget (in US billion)", xlab = "Year after 1960")
nt = length(spending_data$DefenseBudget)
fit <- lm(spending_data$DefenseBudget ~ as.numeric(1:nt))
abline(fit, col ="red")

# added mean
mean(spending_data$DefenseBudget)[1]
abline(h=mean(spending_data$DefenseBudget), col="blue")

# histogram of all points in original data
hist(spending_data$DefenseBudget, label=TRUE, main = "Histogram of U.S Defense Budget", breaks="Sturges", xlab = "Defense Budget (in US billion)")

```

## Testing and training for the original data

I created training and testing sets for spending_data$DefenseBudget, with the last 6 datapoints as the testing set. I chose 6 datapoints because 10% of the observations is around 6.1.

For visualization, I plotted the time series plot for the training set, along with trend and mean.

```{r}
# Define training and testing sets
training = spending_data$DefenseBudget[c(1:51)]
testing  = spending_data$DefenseBudget[c(52:61)]

# time series plot in training with trend and mean
plot.ts(training, ylab = "Defense budget (in US billion)", xlab = "Year after 1960")
fit <- lm(training ~ as.numeric(1:length(training)))
abline(fit, col="red")
abline(h=mean(training), col = "blue")
```

Immediate observations: There seems to be a linear trend that is positive but there is no seasonality and no apparent sharp change in behavior. There is non-constant variance and mean. The histogram seems to be skewed right

# Transformation

Next, since the original data looked skewed, I wanted to see if I needed to make any necessary transformations to make the model stationary and to stabilize the variance.

```{r}
t <- 1:length(training)
fit <- lm(training~t)
bcTransform <- boxcox(training ~ t, plotit=TRUE)
lambda = bcTransform$x[which(bcTransform$y == max(bcTransform$y))]
train.bc = (1/lambda)*(training^lambda-1)
```

Using the Box-Cox transformation, the BcTransform command gave me the value of lambda to be around 0.1414. Then I named the new transformed data using the lambda value called spent.

```{r}
# comparison of best lambda vs log/sqrt transformation
training.log = log(training)
training.sqrt = sqrt(training)

# compare transforms on time series plot
op=par(mfrow=c(2,2))
plot.ts(training, main = "Original Training set")
plot.ts(train.bc, main = "Box-cox Transform")
plot.ts(training.log, main = "Log transform")
plot.ts(training.sqrt, main = "Square root transform")
```

For comparison, I created log and square root transformations as well. In order to determine which model to use, I will create histograms of the original model vs the transformed model (spent).

```{r}
# histogram of original vs transformed
op=par(mfrow=c(1,2))
hist(train.bc, col = "light blue", main = "Histogram of Transformed Data")
hist(training, col = "light blue", main = "Histogram of Trained Original Data")
hist(training.log)
```

Based from the histogram, the spent dataset looked more symmetric while the original still looked skewed right. Variance also looked more stable in the transformation. As a result, the spent data looks more appropriate to use.

## Differencing

Then, I want to check if spent needs any differencing. To start, I plotted the time series for the training set for spent differenced once, then twice, then three times at lag 1. As I increased difference each time, I plotted its time series along with its mean and trend for comparison. 

```{r}
dat <- diff(training.log, 1)
plot.ts(dat, main = "training differenced once at lag 1", type= "l")
fit <- lm(dat ~ as.numeric(1:length(dat)))
abline(fit, col = "red")
abline(h=mean(dat), col = "blue")

dat.2 <- diff(dat, 1)
plot.ts(dat.2, main = "", type= "l")
dat.3 <- diff(dat.2,1)
plot(dat.3, type= "l")
```

Furthermore, I checked the variance for the 3 differences. I found that when differencing twice, the minimum variance was found. At difference 3, the variance increased, so differencing at 2 is most appropriate.

```{r}
var(training.log)
var(dat)
var(dat.2)
var(dat.3)
```

Since I will difference at 2, I will check the plot for dat.2, which looks somewhat stationary.

## Model Identification

For comparison, I plotted ACF and PACF for the training set of spent, spent when differenced once, and spent when differenced twice. I will compare histograms of the training, dat, dat.2.

```{r}

# lag.max mean anything ????

opar <- par(no.readonly = T)
par(mfrow=c(2,1))
par(mar=c(4,4,4,4))
acf(training.log , lag.max=40, main = "")
pacf(training.log, lag.max=40)
par(opar)

opar <- par(no.readonly = T)
par(mfrow=c(2,1))
acf(dat, lag.max = 40, main="Sample acf", ylim=c(-1,1),xlab="h", ylab= expression(hat(rho)[X](h)))
pacf(dat, lag.max = 40, main="Sample pacf", ylim=c(-1,1),xlab="h", ylab= expression(hat(rho)[X](h)))
# run ACF(9) and see if coefficient is 0
# sarima (p=P=1)

opar <- par(no.readonly = T)
par(mfrow=c(2,1))
acf(dat.2, lag.max=40, main="Sample acf", ylim=c(-1,1),xlab="h", ylab= expression(hat(rho)[X](h)))
pacf(dat.2, lag.max=40, main="Sample pacf", ylim=c(-1,1),xlab="h", ylab= expression(hat(rho)[X](h)))
```

At dat.2, I noticed that for ACF, lag 9 was the furthest-most lag outside of confidence interval, and lag 1 is outside as well. For PACF, the same could be said, with lags 1 and 9 outside confidence interval

```{r}
hist(training.log, col = "light blue", xlab = "", main = "Histogram of original data")
hist(dat, col = "light blue", xlab = "", main = "Histogram of data differenced once")
hist(dat.2, col = "light blue", xlab = "", main = "Histogram of data differenced twice")

#### what this mean  ?????
hist(dat.2, density=20, breaks =20, col = "blue", prob=T)
m <- mean(dat.2)
std <- sqrt(var(dat.2))
curve(dnorm(x,m,std), add = T)
```

From the histogram, the original data seemed skewed right. dat.1 looked less skewed, but overall still skewed right. dat.2 looks somewhat Gaussian

## Model Estimation

Here, I will estimate coefficients of p and q. I used a for loop function to find the lowest AICc. This for loop will produce candidate models for me to run diagnostics on.

```{r}
df <- expand.grid(p=0:11, q=0:9) 
df <- cbind(df, AICc=NA)

for (i in 1:nrow(df)) {
  sarima.obj <- NULL
  try(arima.obj <- arima(training.log, order=c(df$p[i],1, df$q[i]), method="ML"))
  if (!is.null(arima.obj)) { df$AICc[i] <- AICc(arima.obj) }
  # print(df[i, ])
}
# this prints out the model with lowest AICc
df[which.min(df$AICc), ]

# this prints all AICc values, starting from the lowest to highest
df[order(df$AICc),]
```

Using principle of parsimony and the lowest AICC, I will consider the two best models which are (0,1) and (4,2) where p and q are respectively shown. In model 1, I found the MA coefficient to be -1, which meant I may have overdifferenced. As a result, I will revert back to differencing only once. This new model will be called new_fit1. Model 1 will use order at (1,1,1) and model 2 will use order at (0,2,1) called new_fit1 and fit2 respectively.

```{r}
# Final model 1
fit1 <- arima(training.log, order = c(1,1,0), method = "ML")
fit1
AICc(fit1)

new_fit1 <- arima(training.log, order = c(1,1,0), seasonal = list (order = c(1,0,0), period = 9), method = "ML")
new_fit1
AICc(new_fit1)


# rerun the for loop
# stationary because less than abs 1, and all AR are invertible

# Final model 2
fit2 <- arima(training.log, order = c(0,1,8), fixed = c(0,0,0,0,0,0,0,NA), method = "ML")
fit2
AICc(fit2)

fit2 <- arima(training.log, order = c(0,1,8), method = "ML")
fit2
AICc(fit2)


# stationary because less than abs 1, and all AR are invertible

fit3 <- arima(training.log, order = c(2,1,1), method = "ML")
fit3
AICc(fit3)


# install.packages("UnitCircle")
#library(UnitCircle)
#uc.check(pol_ = c(0,0.238), plot_output = T)

```

The AICc before overdifferencing was at 26.86, but after removing differencing once, the AICc increased to 27.3. Meanwhile, Model 2 AICc was 26.86, which is now lowest of the two candidate models.

Model 1 is invertible because absolute value of coefficients for AR1 and MA1 are less than 1. Model 2 is stationary because it is purely MA. Model 2 is invertible because all roots are outside unit circle.

## Model Diagnostics for model 1

This will be model diagnostics for model 1: order = (1,1,1). For testing the independence of residuals, lag 8 is chosen because square root of observations (61) is around 8.

```{r}
# residual plots
res <- residuals(new_fit1)
mean(res)
var(res)

# layout
par(mfrow=c(1,1))
ts.plot(res, main  = "Fitted Residuals")
t <- 1:length(res)
new_fit1.res = lm(res~t)
abline(new_fit1.res)
abline(h=mean(res), col = "blue")

# Testing independence
Box.test(res, lag = sqrt(61), type = c("Box-Pierce"), fitdf = 1)
Box.test(res, lag = sqrt(61), type = c("Ljung-Box"), fitdf = 1)
Box.test(res^2, lag = sqrt(61), type = c("Ljung-Box"), fitdf = 0)

# test for normality of residuals
shapiro.test(res)

# Histogram and qq plot
par(mfrow=c(1,2))
hist(res, main= "Histogram")
qqnorm(res)
qqline(res, col = "blue")
```

Model 1 passes all tests and residuals are normal. ACF of residuals^2 shows nonlinear dependence

```{r}
# ACF and PACF of residuals
par(mfrow=c(1,2))
acf(res, main = "Autocorrelation")
pacf(res, main = "Partial Autocorrelation")
```

The ACF and PACF of residuals showed lag 9 to be outside confidence interval. Therefore, the residuals follow either a AR(9), MA(9), or ARMA(9,9) model.


From the three models, AR(9) produced the smallest AICc.....

```{r}
# yule-walker test
ar(res, aic=TRUE, order.max = NULL, method=c("yule-walker"))
```

The fitted residuals are to AR(0), so it is white noise. This passes diagnostic checking for model 1.








## Model Diagnostics for model 2

This will be model diagnostics for model 1: order = (0,2,1)

```{r}
# residual plots
res2 <- residuals(fit3)
mean(res2)
var(res2)

# layout
par(mfrow=c(1,1))
ts.plot(res2, main  = "Fitted Residuals")
t <- 1:length(res2)
fit3.res2 = lm(res2~t)
abline(fit3.res2)
abline(h=mean(res2), col = "blue")

# Testing for independence of residuals
Box.test(res2, lag = 8, type = c("Box-Pierce"), fitdf = 3)
Box.test(res2, lag = 8, type = c("Ljung-Box"), fitdf = 3)
Box.test(res2^2, lag = 8, type = c("Ljung-Box"), fitdf = 0)

# test for normality of residuals
shapiro.test(res2)

# Histogram and qq plot
par(mfrow=c(1,2))
hist(res2, main= "Histogram")
qqnorm(res2)
qqline(res2, col = "blue")
```

Model 2 passes all tests and residuals are normal. ACF of residuals\^2 shows nonlinear dependence

```{r}
# ACF and PACF for residuals
par(mfrow=c(1,2))
acf(res2, main = "Autocorrelation")
pacf(res2, main = "Partial Autocorrelation")
```

ACF and PACF showed lag 9 to be outside confidence interval. Therefore, the residuals follow either a AR(9), MA(9), or ARMA(9,9) model, just like model 1.

```{r}
# # AR (9)
# fit_first <- arima(res2, order=c(9,2,0), fixed=c(0,0,0,0,0,0,0,NA,NA))
# acf(residuals(first_fit))
# pacf(residuals(first_fit))
# 
# # MA (9)
# fit_second <- arima(res2, order=c(0,2,9), fixed=c(0,0,0,0,0,0,0,NA,NA), method="ML")
# acf(residuals(fit_second))
# pacf(residuals(fit_second))
# 
# # ARMA(9,9)
# fit_3 <- arima(res2, order=c(9,2,9), fixed=c(0,0,0,0,0,0,0,0,NA, 0,0,0,0,0,0,0,0,NA))

```

From the three models, AR(9) produced the smallest AICc....

```{r}
# yule-walker test
ar(res2, aic=TRUE, order.max = NULL, method=c("yule-walker"))
```





## Best model

Anaylsis of residuals satisfactory?? Since both model 1 and 2 passed diagnostic checking, I will use model 2 simply due to prinicple of parsimony.

```{r}
new_fit1
# $(1-B)^2(1+0.541B^9)Xt$ = (1-0.45B)Zt, Zt ~ WN(0, 155)
```

Final model should be $(1-B)(1+0.541B^9)X_t = (1-0.45B)Z_t, Z_t ~ WN(0, 155)$

## Data Forecasting

In the final step of forecasting, I will predict 10 future observations (colored in red) and plot it against the testing set of the original data (colored in blue) for comparison.

```{r}
# Predict 10 future observations on data and plot
par(mfrow=c(1,1))
m <- length(training)
mypred <- predict(new_fit1, n.ahead=15)

ts.plot(training, xlim=c(1, length(training)+15), ylim=c(min(training), max(mypred$pred +2*mypred$se)) , xlab = "Years after 1960", ylab = "Billions of USD")

points((m+1):(m+15), col="red", mypred$pred)
points((m+1):(m+10), col="blue", testing)
lines((mypred$pred +2*mypred$se), col = "blue", lty= "dashed")
lines((mypred$pred -2*mypred$se), col = "blue", lty= "dashed")


par(mfrow=c(1,1))
m <- length(training)
mypred <- predict(new_fit1, n.ahead=15)

ts.plot(training, xlim=c(1, length(training)+15), ylim=c(min(training), 2000) , xlab = "Years after 1960", ylab = "Billions of USD")

points((m+1):(m+15), col="red", exp(mypred$pred))
points((m+1):(m+10), col="blue", testing)
lines((exp(mypred$pred +2*mypred$se)), col = "blue", lty= "dashed")
lines((exp(mypred$pred -2*mypred$se)), col = "blue", lty= "dashed")


par(mfrow=c(1,1))
m <- length(training)
mypred <- predict(new_fit1, n.ahead=10)
ts.plot(c(lambda*training +1)^(1/lambda), xlim=c(1, length(lambda*training)+10), ylim=c(100,200000000) , xlab = "Years after 1960", ylab = "Billions of USD")

points((m+1):(m+10), col="red", (lambda*mypred$pred +1)^(1/lambda))
points((m+1):(m+10), col="blue", (lambda*testing +1)^(1/lambda))
lines((m+1):(m+10), 
      (lambda*mypred$pred + 1.96*mypred$se+1)^(1/lambda), lty=2)
lines((m+1):(m+10), 
      (lambda*mypred$pred - 1.96*mypred$se+1)^(1/lambda), lty=2)

# original data
plot(spending_data$Year, spending_data$DefenseBudget, ylab = "Defense budget (in US billion)", xlab = "Year", type="l")
```

## Conclusion

Model 1 and 2 both passed diagnostic checking but .. Ultimately, my predicted values followed a straight linear line that would have shown increased defense funding. But, the testing dataset pointed a curve instead. Individuals who helped me on the project include Sunpeng, Youhong, and Raisa Feldman.

## References

I used this site to obtain my original dataset: <https://www.kaggle.com/brandonconrady/us-military-spending-by-year-1960-2020/version/1>

## Appendix

```{r get-labels, echo = FALSE}
labs = knitr::all_labels()
labs = setdiff(labs, c("setup", "get-labels"))
```

```{r all-code, ref.label=labs, eval=FALSE}
```
