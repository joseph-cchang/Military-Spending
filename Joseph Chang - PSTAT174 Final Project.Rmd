---
title: "PSTAT174 Final Project"
author: "Joseph Chang"
date: "3/11/2022"
output:
  html_document:
    code_folding: hide
df_print: paged
---

```{r setup, include=FALSE, echo = FALSE}
# set global chunk options: images will be 7x5 inches
knitr::opts_chunk$set(fig.width=7, fig.height=5, echo =TRUE, message=FALSE, warning = FALSE)
options(digits = 4)
```

```{r}
# library functions
library(knitr)
library(tidyverse) 
library(ISLR) 
library(ROCR)
library(dplyr)
library(tinytex)
library(MASS)
devtools::install_github("FinYang/tsdl")
library(tsdl)
library(forecast)
library(astsa)
library(qpcR)
```

## Data Importation

First, I read in the data and plotted a time series model for it. Additionally, I created a histogram to visualize the Defense Budget

```{r}
spending_data <- read.csv("/Users/josephchang/Desktop/MilitarySpending.csv.xls")

plot(spending_data$Year, spending_data$DefenseBudget, ylab = "Defense budget (in US billion)", xlab = "Year", type="l")

plot.ts(spending_data$Year, spending_data$DefenseBudget, ylab = "Defense budget (in US billion)", xlab = "Year", type="l")

hist(spending_data$DefenseBudget, label=TRUE, main = "Histogram of US Defense Budget", breaks="Sturges", xlab = "Defense Budget (in US billion)")
```

Immediate observations: There seems to be a linear trend that is positive but there is no seasonality and no apparent sharp change in behavior. The histogram seems to be skewed right


# Transformation

Next, since the original data looked skewed, I will make any necessary transformations to make the model stationary. I used Box-Cox transformation and found the best lambda to be 0.30. Then I created a new transformed data called spent using the best lambda. For comparison, I created log and square root transformations as well. In order to determine which model to use, I created histograms of the original model vs the spent. As a result, the spent data looked to be more appropriate. 

```{r}
t <- 1:length(spending_data$DefenseBudget)
fit <- lm(spending_data$DefenseBudget~t)
bcTransform <- boxcox(spending_data$DefenseBudget ~ t, plotit=TRUE)

lambda = bcTransform$x[which(bcTransform$y == max(bcTransform$y))]
spent = (1/lambda)*(spending_data$DefenseBudget^lambda-1)

# comparison of best lambda vs log/sqrt transformation
spending_data.log = log(spending_data$DefenseBudget)
spending_data.sqrt = log(spending_data$DefenseBudget)

# compare transforms on time series plot
op=par(mfrow=c(2,2))
ts.plot(spending_data$DefenseBudget, main = "Original Time Series")
ts.plot(spent, main = "Box-cox Transform")
ts.plot(spending_data.log, main = "Log transform")
ts.plot(spending_data.sqrt, main = "Square root transform")

hist(spent)
hist(spending_data$DefenseBudget)
```

Based from the histogram, the spent dataset looked more normal while the original still looked skewed. Thus, I will use spent.  

## Differencing

Then, I want to check if spent needs any differencing. I created training and testing sets for spent, with the last 10 datapoints as the testing set. I wanted to check the first 3 differences for spent. As I increased difference by 1 each time on the training set for spent, I plotted its time series for comparison. Furthermore, I checked the variance for the 3 differences, and found at difference 2, the minimum variance was found. At difference 3, the variance increased, meaning taking difference at 2 was the most appropriate.

```{r}
length(spent)
spent.train = spent[c(1:51)]
spent.test  = spent[c(52:61)]

dat <- diff(spent.train, 1)
plot.ts(dat, type= "l")
dat.2 <- diff(dat, 1)
plot.ts(dat.2, type= "l")
dat.3 <- diff(dat.2,1)
plot(dat.3, type= "l")

var(spent.train)
var(dat)
var(dat.2)
var(dat.3)
```
From the plot for differencing at 2, the model looks stationary.

## Model Identification

For comparison, I plotted ACF/PACF for training set of spent, the differencing of spent once, and the differencing of spent twice. At the differencing of spent twice, I noticed that lag 9 was the furthest-most lag outside of confidence interval for ACF. The same could be said for PACF.

```{r}
opar <- par(no.readonly = T)
par(mfrow=c(2,1))
acf(spent.train, lag.max=100)
pacf(spent.train, lag.max=100)
par(opar)

opar <- par(no.readonly = T)
par(mfrow=c(2,1))
acf(dat, lag.max = 100, main="Sample acf", ylim=c(-1,1),xlab="h", ylab= expression(hat(rho)[X](h)))
pacf(dat, lag.max=100, main="Sample pacf", ylim=c(-1,1),xlab="h", ylab= expression(hat(rho)[X](h)))

opar <- par(no.readonly = T)
par(mfrow=c(2,1))
acf(dat.2, lag.max=100, main="Sample acf", ylim=c(-1,1),xlab="h", ylab= expression(hat(rho)[X](h)))
pacf(dat.2, lag.max=100, main="Sample pacf", ylim=c(-1,1),xlab="h", ylab= expression(hat(rho)[X](h)))
```

## Model Estimation

Here, I will estimate coefficients of p and q. I used a for loop function to find the lowest AICc. This for loop will produce candidate models for me to run diagnostics on. 

```{r}
df <- expand.grid(p=0:10, q=0:10) 
df <- cbind(df, AICc=NA)

for (i in 1:nrow(df)) {
  sarima.obj <- NULL
  try(arima.obj <- arima(spent.train, order=c(df$p[i],2, df$q[i]), method="ML"))
  if (!is.null(arima.obj)) { df$AICc[i] <- AICc(arima.obj) }
  # print(df[i, ])
}
df[which.min(df$AICc), ]
df[order(df$AICc),]

# Final model 1
fit1 <- arima(spent.train, order = c(1,2,1), method = "ML")
fit1
new_fit1 <- arima(spent.train, order = c(1,1,1), method = "ML")
new_fit1

# Final model 2
fit2 <- arima(spent.train, order = c(0,2,1), method = "ML")
fit2

#source("plot.roots.R")
# look at gauchospace, specify the path
#plot.roots(NULL,polyroot(c(1, -0.3353, 0, -0.1612)), main="(A) roots of ma part, nonseasonal ")
```

Using principle of parsimony and the lowest AICC, I will consider the two best models which are (1,1) and (0,1) where p and q are respectively shown. In model 1, I found the MA coefficient to be -1, which meant I may have overdifferenced. As a result, I will revert back to differencing only once. This new model will be called new_fit1. Model 1 will use order at (1,1,1) and model 2 will use order at (0,2,1) with new_fit1 and fit2 named respectively.

## Model Diagnostics for model 1

This will be model diagnostics for model 1: order = (1,1,1)

```{r}
# residual plots
res <- residuals(new_fit1)
mean(res)
var(res)

# layout
par(mfrow=c(1,1))
ts.plot(res, main  = "Fitted Residuals")
t <- 1:length(res)
new_fit1.res = lm(res~t)
abline(fit1.res)
abline(h=mean(res), col = "blue")

# ACF and PACF
par(mfrow=c(1,2))
acf(res, main = "Autocorrelation")
pacf(res, main = "Partial Autocorrelation")
# ACF and PACF show lag 9 to be outside confidence interval. Therefore, the residuals follow either a AR(9), MA(9), or ARMA(9,9) model.

# AR (9)
first_fit <- arima(res, order=c(9,0,0), fixed=c(0,0,0,0,0,0,0,0,NA, NA))
acf(residuals(first_fit))
pacf(residuals(first_fit))

# MA (9)
second_fit <- arima(res, order=c(0,1,9), fixed=c(0,0,0,0,0,0,0,0,NA))

# ARMA(9,9)
third_fit <- arima(res, order=c(9,1,9), fixed=c(0,0,0,0,0,0,0,0, NA, 0,0,0,0,0,0,0,0,NA))

# From the three models, AR(9) produced the smallest AICc

# Testing for independence of residuals
# lag 8 is chosen since sqrt 61 is around 8
Box.test(res, lag = 8, type = c("Box-Pierce"), fitdf = 2)

Box.test(res, lag = 8, type = c("Ljung-Box"), fitdf = 2)

Box.test(res^2, lag = 8, type = c("Ljung-Box"), fitdf = 0)

# test for normality of residuals
shapiro.test(res)

# yule-walker test
ar(res, aic=TRUE, order.max = NULL, method=c("yule-walker"))

# Histogram and qq plot
par(mfrow=c(1,2))
hist(res, main= "Histogram")
qqnorm(res)
qqline(res, col = "blue")

# Model 1 passes all tests and residuals are normal
```


## Model Diagnostics for model 2

This will be model diagnostics for model 1: order = (0,2,1)

```{r}
# residual plots
res2 <- residuals(fit2)
mean(res2)
var(res2)

# layout
par(mfrow=c(1,1))
ts.plot(res2, main  = "Fitted Residuals")
t <- 1:length(res2)
fit2.res2 = lm(res2~t)
abline(fit2.res2)
abline(h=mean(res2), col = "blue")

# ACF and PACF
par(mfrow=c(1,2))
acf(res2, main = "Autocorrelation")
pacf(res2, main = "Partial Autocorrelation")

# ACF and PACF show lag 9 to be outside confidence interval. Therefore, the residuals follow either a AR(9), MA(9), or ARMA(9,9) model.

# AR (9)
fit_1 <- arima(res2, order=c(9,2,0), fixed=c(0,0,0,0,0,0,0,0, NA))

# MA (9)
fit_2 <- arima(res2, order=c(0,2,9), fixed=c(0,0,0,0,0,0,0,0,NA))

# ARMA(9,9)
fit_3 <- arima(res2, order=c(9,2,9), fixed=c(0,0,0,0,0,0,0,0,NA, 0,0,0,0,0,0,0,0,NA))

# From the three models, AR(9) produced the smallest AICc

# Testing for independence of residuals
Box.test(res2, lag = 8, type = c("Box-Pierce"), fitdf = 1)

Box.test(res2, lag = 8, type = c("Ljung-Box"), fitdf = 1)

Box.test(res2^2, lag = 8, type = c("Ljung-Box"), fitdf = 0)

# test for normality of residuals
shapiro.test(res2)

# yule-walker test
ar(res2, aic=TRUE, order.max = NULL, method=c("yule-walker"))

# Histogram and qq plot
par(mfrow=c(1,2))
hist(res2, main= "Histogram")
qqnorm(res2)
qqline(res2, col = "blue")


# residuals pass Box.test and residuals are normal
```

```{r}
# I will use fit2 as the final model
fit_1
```

Final model should be 
(1-B)^2(1+0.643*B^9)Xt = Zt, Zt ~ WN(0, 0.255)



## Data Forecasting

```{r}
# Predict 10 future observations and plot
par(mfrow=c(1,1))
m <- length(spent.train)
mypred <- predict(fit_1, n.ahead=10)
ts.plot(c(lambda*spent.train +1)^(1/lambda), xlim=c(1, length(lambda*spent.train)+10), ylim=c(100,1200) , xlab = "Years after 1960", ylab = "Billions of USD")

points((m+1):(m+10), col="red", (lambda*mypred$pred +1)^(1/lambda))
points((m+1):(m+10), col="blue", (lambda*spent.test +1)^(1/lambda))
lines((m+1):(m+10), 
      (lambda*mypred$pred + 1.96*mypred$se+1)^(1/lambda), lty=2)
lines((m+1):(m+10), 
      (lambda*mypred$pred - 1.96*mypred$se+1)^(1/lambda), lty=2)
```



