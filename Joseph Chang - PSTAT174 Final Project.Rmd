---
title: "PSTAT174 Project"
author: "Joseph Chang"
date: "March 11th, 20222"
output: pdf_document
---

```{r setup, include=FALSE, echo = FALSE}
# set global chunk options: images will be 7x5 inches
knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.width=7, fig.height=5, echo =TRUE)
options(digits = 4)

# library functions
library(knitr)
library(tidyverse) 
library(ISLR) 
library(ROCR)
library(dplyr)
library(tinytex)
library(MASS)
devtools::install_github("FinYang/tsdl")
library(tsdl)
library(forecast)
library(astsa)
library(qpcR)
```

## Data Importation
```{r}
# load data
spending_data <- read.csv("/Users/josephchang/Desktop/MilitarySpending.csv.xls")
spending_data

# plot data
plot(spending_data$Year, spending_data$DefenseBudget, ylab = "Defense budget (in US billion)", xlab = "Year", type="l")

# plot time series
plot.ts(spending_data$Year, spending_data$DefenseBudget, ylab = "Defense budget (in US billion)", xlab = "Year", type="l")

# Immediate observations: There seems to be a linear trend that is positive but there is no seasonality and no apparent sharp change in behavior.

hist(spending_data$DefenseBudget)
```

# Transformation
```{r}
# Since original data is skewed I will try Box-Cox transformation
t <- 1:length(spending_data$DefenseBudget)
fit <- lm(spending_data$DefenseBudget~t)
bcTransform <- boxcox(spending_data$DefenseBudget ~ t, plotit=TRUE)

# best lambda
lambda = bcTransform$x[which(bcTransform$y == max(bcTransform$y))]
spent = (1/lambda)*(spending_data$DefenseBudget^lambda-1)

# comparison of best lambda vs log/sqrt transformation
spending_data.log = log(spending_data$DefenseBudget)
plot.ts(spending_data.log)

spending_data.sqrt = log(spending_data$DefenseBudget)
plot.ts(spending_data.sqrt)

# compare transforms on time series plot
op=par(mfrow=c(2,2))
ts.plot(spending_data$DefenseBudget, main = "Original Time Series")
ts.plot(spent, main = "Box-cox Transform")
ts.plot(spending_data.log, main = "Log transform")
ts.plot(spending_data.sqrt, main = "Square root transform")

# histogram of spent vs original
hist(spent)
hist(spending_data$DefenseBudget)
# spent seems more normal while original is skewed

# time series plot of spent vs original
plot.ts(spent)
plot.ts(spending_data$DefenseBudget)
# both plots look around the same, but spent seems to have more spent in the median


# Based from the histogram, I will use the spent instead of the original data 


# training and testing dataset
length(spent)
spent.train = spent[c(1:51)]
spent.test  = spent[c(52:61)]
```

## Differencing
```{r}
# difference once and plot time series
dat <- diff(spent.train, 1)
plot.ts(dat, type= "l")

# twice differenced and plot time series
dat.2 <- diff(dat, 1)
plot.ts(dat.2, type= "l")

# three times differenced and plot
dat.3 <- diff(dat.2,1)
plot(dat.3, type= "l")

# check variance
var(spent.train)
var(dat)
var(dat.2)
var(dat.3)

# I differenced the data and found d to be 2. Based from the variance, the lowest variance is when d=2. At d=3, variance is increased again, therefore I will take d=2.
```


## Model Identification
```{r}
# ACF, PACF for spent
opar <- par(no.readonly = T)
par(mfrow=c(2,1))
acf(spent.train, lag.max=100)
pacf(spent.train, lag.max=100)
par(opar)

# ACF, PACF for dat (difference once)
opar <- par(no.readonly = T)
par(mfrow=c(2,1))
acf(dat, lag.max = 100, main="Sample acf", ylim=c(-1,1),xlab="h", ylab= expression(hat(rho)[X](h)))
pacf(dat, lag.max=100, main="Sample pacf", ylim=c(-1,1),xlab="h", ylab= expression(hat(rho)[X](h)))

# ACF, PACF for dat.2 (difference twice)
opar <- par(no.readonly = T)
par(mfrow=c(2,1))
acf(dat.2, lag.max=100, main="Sample acf", ylim=c(-1,1),xlab="h", ylab= expression(hat(rho)[X](h)))
pacf(dat.2, lag.max=100, main="Sample pacf", ylim=c(-1,1),xlab="h", ylab= expression(hat(rho)[X](h)))
```

## Model estimation
```{r}
# Candidate models:
df <- expand.grid(p=0:10, q=0:10) 
df <- cbind(df, AICc=NA)

# Compute AICc:
for (i in 1:nrow(df)) {
  sarima.obj <- NULL
  try(arima.obj <- arima(spent.train, order=c(df$p[i],2, df$q[i]), method="ML"))
  if (!is.null(arima.obj)) { df$AICc[i] <- AICc(arima.obj) }
  # print(df[i, ])
}
df[which.min(df$AICc), ]
df[order(df$AICc),]

# using principle of parsimony and lowest AICC, I will consider the two best models which are (1,1) and (0,1) where p and q are respectively shown

# Final model 1
fit1 <- arima(spent.train, order = c(1,2,1), 
             method = "ML")
fit1

# Final model 2
fit2 <- arima(spent.train, order = c(0,2,1), 
             method = "ML")
fit2

#source("plot.roots.R")
#plot.roots(NULL,polyroot(c(1, -0.3353, 0, -0.1612)), main="(A) roots of ma part, nonseasonal ")
```

## Model Diagnostics for model 1
```{r}
# residual plots
res <- residuals(fit1)
mean(res)
var(res)

# layout
par(mfrow=c(1,1))
ts.plot(res, main  = "Fitted Residuals")
t <- 1:length(res)
fit1.res = lm(res~t)
abline(fit1.res)
abline(h=mean(res), col = "blue")

# ACF and PACF
par(mfrow=c(1,2))
acf(res, main = "Autocorrelation")
pacf(res, main = "Partial Autocorrelation")
# ACF and PACF show lag 9 to be outside confidence interval. Therefore, the residuals follow either a AR(9), MA(9), or ARMA(9,9) model.

# AR (9)
fit1 <-arima(spent.train, order=c(9,1,0), fixed=c(0,0,0, NA,NA))

# MA (9)
fit2 <-arima(spent.train, order=c(0,1,9), fixed=c(0,0,0, NA,NA))

# ARMA(9,9)
fit3 <-arima(spent.train, order=c(9,1,9), fixed=c(0,0,0, NA,NA))

# Testing for independence of residuals
# lag 8 is chosen since sqrt 61 is around 8
Box.test(res, lag = 8, type = c("Box-Pierce"), fitdf = 2)

Box.test(res, lag = 8, type = c("Ljung-Box"), fitdf = 2)

Box.test(res^2, lag = 8, type = c("Ljung-Box"), fitdf = 0)

# test for normality of residuals
shapiro.test(res)

# yule-walker test
ar(res, aic=TRUE, order.max = NULL, method=c("yule-walker"))

# Histogram and qq plot
par(mfrow=c(1,2))
hist(res, main= "Histogram")
qqnorm(res)
qqline(res, col = "blue")

# Model 1 passes all tests but Mc-Leod Li test and residuals are normal
```


## Model Diagnostics for model 2
```{r}
# residual plots
res2 <- residuals(fit2)
mean(res2)
var(res2)

# layout
par(mfrow=c(1,1))
ts.plot(res2, main  = "Fitted Residuals")
t <- 1:length(res2)
fit2.res2 = lm(res2~t)
abline(fit2.res2)
abline(h=mean(res2), col = "blue")

# ACF and PACF
par(mfrow=c(1,2))
acf(res2, main = "Autocorrelation")
pacf(res2, main = "Partial Autocorrelation")

# Testing for independence of residuals
Box.test(res2, lag = 8, type = c("Box-Pierce"), fitdf = 1)

Box.test(res2, lag = 8, type = c("Ljung-Box"), fitdf = 1)

Box.test(res2^2, lag = 8, type = c("Ljung-Box"), fitdf = 0)

# test for normality of residuals
shapiro.test(res2)

# yule-walker test
ar(res2, aic=TRUE, order.max = NULL, method=c("yule-walker"))

# Histogram and qq plot
par(mfrow=c(1,2))
hist(res2, main= "Histogram")
qqnorm(res2)
qqline(res2, col = "blue")


# residuals pass Box.test and residuals are normal
```

```{r}
# I will use fit2 as the final model
fit2
```

Final model should be 
(1-B^2)Xt = (1-0.49B)Zt, Zt ~ WN(0, 0.0927)



## Data Forecasting

```{r}
# Predict 10 future observations and plot
par(mfrow=c(1,1))
m <- length(spent.train)
mypred <- predict(fit2, n.ahead=10)
ts.plot(c(lambda*spent.train +1)^(1/lambda), xlim=c(1, length(lambda*spent.train)+10), ylim=c(100,1200) , xlab = "Years after 1960", ylab = "Billions of USD")

points((m+1):(m+10), col="red", (lambda*mypred$pred +1)^(1/lambda))
points((m+1):(m+10), col="blue", (lambda*spent.test +1)^(1/lambda))
lines((m+1):(m+10), 
      (lambda*mypred$pred + 1.96*mypred$se+1)^(1/lambda), lty=2)
lines((m+1):(m+10), 
      (lambda*mypred$pred - 1.96*mypred$se+1)^(1/lambda), lty=2)
```



