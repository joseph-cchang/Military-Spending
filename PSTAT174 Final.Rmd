---
title: "PSTAT174 Project"
author: "Joseph Chang"
date: "3/11/2022"
output: pdf_document
---

```{r setup, include=FALSE}
#install.packages(c('tidyverse', 'knitr'))
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(tidyverse) 
library(ISLR) 
library(ROCR)
library(dplyr)
library(tinytex)
library(MASS)
devtools::install_github("FinYang/tsdl")
library(tsdl)
library(forecast)
library(astsa)
library(qpcR)

```

# Project description 
The data set I plan to use is the US military expense since 1960. This is money that the US military has used for the defense budget. Although the dataset has two other columns with population of U.S. and GDP, I think the defense budget is more interesting. The GDP and population follow a constant, linear relationship.

# Project motivations and objectives
This data is interesting to me because I have been studying history, especially US history and the wars it has fought or participated in. This data gives me an idea how much the US government spent during the wars and also in the present day. I plan to address the prediction of the defense budget in the next few years (forecasting).

There seems to be a trend that is positive but there is no seasonality and no apparent sharp change in behavior.

## Data Importation
```{r}
# load data
spending_data <- read.csv("/Users/josephchang/Desktop/MilitarySpending.csv.xls")
spending_data

# plot data
plot(spending_data$Year, spending_data$DefenseBudget, ylab = "Defense budget (in US billion)", xlab = "Year", type="l")

# plot time series
plot.ts(spending_data$Year, spending_data$DefenseBudget, ylab = "Defense budget (in US billion)", xlab = "Year", type="l")

```

# transformation
```{r}
# Box-Cox transformation:
t = 1:length(spending_data$DefenseBudget)
fit = lm(spending_data$DefenseBudget~t)
bcTransform = boxcox(spending_data$DefenseBudget ~ t, plotit=TRUE)

# best lambda
lambda = bcTransform$x[which(bcTransform$y == max(bcTransform$y))]
spent = (1/lambda)*(spending_data$DefenseBudget^lambda-1)

# comparison of best lambda vs log/sqrt transformation
spending_data.log = log(spending_data$DefenseBudget)
plot.ts(spending_data.log)

spending_data.sqrt = log(spending_data$DefenseBudget)
plot.ts(spending_data.sqrt)

# compare transforms on time series plot
op=par(mfrow=c(2,2))
ts.plot(spending_data$DefenseBudget, main = "Original Time Series")
ts.plot(spent, main = "Box-cox Transform")
ts.plot(spending_data.log, main = "Log transform")
ts.plot(spending_data.sqrt, main = "Square root transform")

# histogram of spent vs original
hist(spent)
hist(spending_data$DefenseBudget)
# spent seems more normal while original is skewed

# time series plot of spent vs original
plot.ts(spent)
plot.ts(spending_data$DefenseBudget)
# both plots look around the same, but spent seems to have more spent in the median


# Based from the histogram, I will use the spent instead of the original data 


# training and testing dataset
length(spent)
spent.train = spent[c(1:51)]
spent.test  = spent[c(52:61)]

```

## Differencing
```{r}
# difference once and plot
dat <- diff(spent.train, 1)
dat
plot.ts(dat, type= "l")

# twice differenced and plot
dat.2 <- diff(dat, 1)
dat.2
plot.ts(dat.2, type= "l")

# three times differenced and plot
dat.3 <- diff(dat.2,1)
dat.3
plot(dat.3, type= "l")

# check variance
var(spent.train)
var(dat)
var(dat.2)
var(dat.3)

# I differenced the data and found d to be 2. Based from the variance, the lowest variance is when d=2. At d=3, variance is increased again, therefore I will take d=2.
```


## Model Identification
```{r}
# ACF, PACF for spent
opar <- par(no.readonly = T)
par(mfrow=c(2,1))
acf(spent.train, lag.max=100)
pacf(spent.train, lag.max=100)
par(opar)

# ACF, PACF for dat (difference once)
opar <- par(no.readonly = T)
par(mfrow=c(2,1))
acf(dat, lag.max = 100, main="Sample acf", ylim=c(-1,1),xlab="h", ylab= expression(hat(rho)[X](h)))
pacf(dat, lag.max=100, main="Sample pacf", ylim=c(-1,1),xlab="h", ylab= expression(hat(rho)[X](h)))

# ACF, PACF for dat.2 (difference twice)
opar <- par(no.readonly = T)
par(mfrow=c(2,1))
acf(dat.2, lag.max=100, main="Sample acf", ylim=c(-1,1),xlab="h", ylab= expression(hat(rho)[X](h)))
pacf(dat.2, lag.max=100, main="Sample pacf", ylim=c(-1,1),xlab="h", ylab= expression(hat(rho)[X](h)))

```

## Model estimation
```{r}
# Candidate models:
df <- expand.grid(p=0:10, q=0:10) 
df <- cbind(df, AICc=NA)

# Initially, I found lowest AICc using parameters(9,1). However, the coefficient at MA(1) produced a -1.000 value meaning I may have overdifferenced. Therefore, I will revert back to differencing at d=1.

# Compute AICc:
for (i in 1:nrow(df)) {
  sarima.obj <- NULL
  try(arima.obj <- arima(spent.train, order=c(df$p[i],2, df$q[i]), method="ML"))
  if (!is.null(arima.obj)) { df$AICc[i] <- AICc(arima.obj) }
  # print(df[i, ])
}
df[which.min(df$AICc), ]

order(df$AICc)
df[order(df$AICc),]
# lowest AICc is using parameters (0, 8). However, this is still too large for lag.
# using principle of parsimony and lowest AICC, I will consider the two best models which are (1,1) and (1,0) where p and q are respectively shown

# Final model 1
fit1 <- arima(spent.train, order = c(1,1,1), 
             method = "ML")
fit1

# Final model 2
fit2 <- arima(spent.train, order = c(1,1,0), 
             method = "ML")
fit2

source("plot.roots.R")
plot.roots(NULL,polyroot(c(1, -0.3353, 0, -0.1612)), main="(A) roots of ma part, nonseasonal ")
```

## Model Diagnostics
```{r}

# for fit1
# residual plots
res <- residuals(fit1)
mean(res)
var(res)

# layout
par(mfrow=c(1,1))
ts.plot(res, main  = "Fitted Residuals")
t <- 1:length(res)
fit1.res = lm(res~t)
abline(fit1.res)
abline(h=mean(res), col = "blue")

# ACF and PACF
par(mfrow=c(1,2))
acf(res, main = "Autocorrelation")
pacf(res, main = "Partial Autocorrelation")

# Testing for independence of residuals
Box.test(res, lag = 8, type = c("Box-Pierce"), fitdf = 2)

Box.test(res, lag = 8, type = c("Ljung-Box"), fitdf = 2)

Box.test(res^2, lag = 8, type = c("Ljung-Box"), fitdf = 0)

# test for normality of residuals
shapiro.test(res)

# Histogram and qq plot
par(mfrow=c(1,2))
hist(res, main= "Histogram")
qqnorm(res)
qqline(res, col = "blue")


# residuals pass Box.test and residuals are normal


# for fit2
# residual plots
res2 <- residuals(fit2)
mean(res2)
var(res2)

# layout
par(mfrow=c(1,1))
ts.plot(res2, main  = "Fitted Residuals")
t <- 1:length(res2)
fit2.res2 = lm(res2~t)
abline(fit2.res2)
abline(h=mean(res2), col = "blue")

# ACF and PACF
par(mfrow=c(1,2))
acf(res2, main = "Autocorrelation")
pacf(res2, main = "Partial Autocorrelation")

# Testing for independence of residuals
Box.test(res2, lag = 8, type = c("Box-Pierce"), fitdf = 1)

Box.test(res2, lag = 8, type = c("Ljung-Box"), fitdf = 1)

Box.test(res2^2, lag = 8, type = c("Ljung-Box"), fitdf = 0)

# test for normality of residuals
shapiro.test(res2)

# Histogram and qq plot
par(mfrow=c(1,2))
hist(res2, main= "Histogram")
qqnorm(res2)
qqline(res2, col = "blue")


# residuals pass Box.test and residuals are normal
fit1
```

Final model should be 
(1-0.8762B)(1-B)Xt = (1-0.375B)Zt, Zt ~ WN(0, 0.08775)

## Data Forecasting

```{r}
# Predict 10 future observations and plot
par(mfrow=c(1,1))
m <- length(spent.train)
mypred <- predict(fit1, n.ahead=10)
ts.plot(c(lambda*spent.train +1)^(1/lambda), xlim=c(1, length(lambda*spent.train)+10), ylim=c(100,1200) , xlab = "Years after 1960", ylab = "Billions of dollars")
points((m+1):(m+10), col="red", (lambda*mypred$pred +1)^(1/lambda))
points((m+1):(m+10), col="blue", (lambda*spent.test +1)^(1/lambda))
lines((m+1):(m+10), 
      (lambda*mypred$pred + 1.96*mypred$se+1)^(1/lambda), lty=2)
lines((m+1):(m+10), 
      (lambda*mypred$pred - 1.96*mypred$se+1)^(1/lambda), lty=2)
```

need validation... test and train 



fit model, (for loop) using aICC, find 2 best models, do diagnostics ( boxtest,), bestmodel passes, forecasting


